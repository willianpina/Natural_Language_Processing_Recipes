{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<h1 align=center>Processamento Avançado de Linguagem Natural</h1>\n",
    "<p align=center><img src= https://imagens.itforum.com.br/itforum.com.br/wp-content/uploads/2021/05/AdobeStock_230030937-1-scaled.jpeg width=500></p>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Este capítulo aborda várias técnicas avançadas de NLP e utiliza algoritmos de aprendizado de máquina para extrair informações de dados de texto e aplicativos avançados de NLP com uma abordagem de solução e implementação.\n",
    "* Receita 1. Extração de frases nominais\n",
    "* Receita 2. Similaridade de texto\n",
    "* Receita 3. Marcação de classes gramaticais\n",
    "* Receita 4. Extração de informações – NER – reconhecimento de entidade\n",
    "* Receita 5. Modelagem de tópicos\n",
    "* Receita 6. Classificação de texto\n",
    "* Receita 7. Análise de sentimentos\n",
    "* Receita 8. Desambiguação do sentido da palavra\n",
    "* Receita 9. Reconhecimento de fala e fala para texto\n",
    "* Receita 10. Texto para fala\n",
    "* Receita 11. Detecção e tradução de linguagem\n",
    "\n",
    "Antes de entrar nas receitas, vamos primeiro entender o pipeline e o ciclo de vida do NLP. Há muitos conceitos implementados neste livro e você pode ficar sobrecarregado com o conteúdo. Para torná-lo mais simples e suave, vamos ver o fluxo que você precisa seguir para uma solução de NLP.\n",
    "\n",
    "Por exemplo, vamos considerar a análise e previsão do sentimento do cliente para um produto, marca ou serviço.\n",
    "* **Defina o problema**. Entenda o sentimento do cliente em relação aos produtos.\n",
    "* **Entenda a profundidade e amplitude do problema**. Entenda os sentimentos do cliente/usuário em relação ao produto. Por que estamos fazendo isso? Qual é o impacto nos negócios?\n",
    "* **Faça um brainstorming de requisitos de dados**. Tenha uma atividade de brainstorming para listar todos os pontos de dados possíveis.\n",
    "    * Todos os comentários de clientes em plataformas de comércio eletrônico como Amazon, Flipkart e assim por diante\n",
    "    * E-mail enviado por clientes\n",
    "    * Formulários de reivindicação de garantia\n",
    "    * Dados de pesquisa\n",
    "    * Conversas de call center usando voz para texto\n",
    "    * Formulários de feedback\n",
    "    * Dados de mídia social como Twitter, Facebook e LinkedIn\n",
    "* **Coleta de dados**: Você aprendeu diferentes técnicas para coletar os dados no Capítulo 1. Com base nos dados e no problema, pode ser necessário incorporar diferentes métodos de coleta de dados. Nesse caso, você pode usar web scraping e APIs do Twitter.\n",
    "* **Pré-processamento de texto**: você sabe que os dados nem sempre estarão limpos. Você precisa gastar uma quantidade significativa de tempo processando-o e extraindo insights usando os métodos discutidos no Capítulo 2.\n",
    "* **Text to feature**: O texto é composto de caracteres e as máquinas têm dificuldade em entendê-los. Usando qualquer um dos métodos que você aprendeu nos capítulos anteriores, você os converte em recursos que máquinas e algoritmos podem entender.\n",
    "* **Aprendizado de máquina/aprendizagem profunda**: O aprendizado de máquina e o aprendizado profundo fazem parte de um guarda-chuva de inteligência artificial que faz com que os sistemas aprendam padrões automaticamente nos dados sem serem programados. A maioria das soluções de NLP são baseadas nisso. Como o texto é convertido em recursos, você pode aproveitar algoritmos de aprendizado de máquina ou aprendizado profundo para atingir objetivos como classificação de texto e geração de linguagem natural.\n",
    "* **Insights e implantação**: Não adianta criar soluções de NLP sem que os insights adequados sejam comunicados ao negócio. Sempre reserve um tempo para conectar os pontos entre a saída do modelo/análise e o negócio, criando assim o máximo impacto."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Receita 4-1. Extraindo o substantivo da frase\n",
    "Esta receita extrai o substantivo da frase dos dados de texto (uma frase ou os documentos).\n",
    "### Problema\n",
    "Você deseja extrair o substantivo de uma frase.\n",
    "### Solução\n",
    "A extração de substantivos das frases é importante quando você deseja analisar quem em uma frase. Vejamos um exemplo usando TextBlob.\n",
    "### Como funciona\n",
    "Execute o código a seguir para extrair o substantivo das frases."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "john\n",
      "natural language processing\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from textblob import TextBlob\n",
    "#Extract noun\n",
    "blob = TextBlob(\"John is learning natural language processing\")\n",
    "for np in blob.noun_phrases:\n",
    "    print(np)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Receita 4-2. Encontrando semelhança entre textos\n",
    "Esta receita discute como encontrar a semelhança entre dois documentos ou texto. Existem muitas métricas semelhantes, como euclidiana, cosseno e Jaccard. Aplicações de similaridade de texto podem ser encontradas em correção ortográfica, desduplicação de dados, triagem de currículos, aplicativos de pesquisa em vários domínios e sistema de recomendação baseado em conteúdo.\n",
    "Aqui estão algumas das medidas de similaridade.\n",
    "* **Similaridade do cosseno**: Calcula o cosseno do ângulo entre os dois vetores.\n",
    "* **Similaridade de Jaccard**: A pontuação é calculada a partir da interseção ou união de palavras.\n",
    "* **Índice de Jaccard**: (o número em ambos os conjuntos) / (o número em qualquer um dos conjuntos) * 100.\n",
    "* **Distância de Levenshtein**: Um número mínimo de inserções, exclusões e substituições é necessário para transformar a string a na string b.\n",
    "* **Hamming distance**: O número de posições com o mesmo símbolo em ambas as strings. Ele pode ser definido apenas para strings com comprimento igual. Você deseja encontrar as semelhanças entre texto e documentos.\n",
    "\n",
    "### Solução\n",
    "A maneira mais simples de fazer isso é usando a similaridade de cosseno da biblioteca sklearn.\n",
    "### Como funciona\n",
    "Siga as etapas nesta seção para calcular a pontuação de similaridade entre documentos de texto.\n",
    "#### Etapa 2-1. Criar/ler os dados de texto\n",
    "Aqui estão os dados."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "documents = (\n",
    "    \"I like NLP\",\n",
    "    \"I am exploring NLP\",\n",
    "    \"I am a beginner in NLP\",\n",
    "    \"I want to learn NLP\",\n",
    "    \"I like advanced NLP\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Etapa 2-2. Encontrar semelhanças\n",
    "Execute o código a seguir para encontrar a semelhança."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "(5, 10)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "#Compute tfidf : feature engineering(refer previous chapter – Recipe 3-4)\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "tfidf_matrix.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1.        , 0.17682765, 0.14284054, 0.13489366, 0.68374784]])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#compute similarity for first sentence with rest of the sentences\n",
    "cosine_similarity(tfidf_matrix[0:1],tfidf_matrix)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A primeira frase e a última frase têm maior semelhança em comparação com o resto das frases."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Receita 4-3. Marcando parte do discurso\n",
    "A marcação de parte do discurso (POS) é outra parte crucial do processamento de linguagem natural que envolve rotular as palavras com uma parte do discurso, como substantivo, verbo, adjetivo e assim por diante. POS é a base para resolução de entidade nomeada, resposta a perguntas e desambiguação de sentido de palavra.\n",
    "### Problema\n",
    "Você deseja marcar as partes do discurso em uma frase.\n",
    "### Solução\n",
    "Há duas maneiras de construir um tagger.\n",
    "* **Rule-based**: Regras criadas manualmente, que marcam uma palavra pertencente a um determinado POS.\n",
    "* **Com base estocástica**: Esses algoritmos capturam a sequência das palavras e marcam a probabilidade da sequência usando modelos ocultos de Markov.\n",
    "\n",
    "### Como Funciona\n",
    "Mais uma vez, o NLTK tem o melhor módulo de etiquetagem POS. **nltk.pos_tag(word)** é a função que gera a marcação POS para qualquer palavra. Use **for loop** e gere POS para todas as palavras presentes no documento.\n",
    "\n",
    "#### Etapa 3-1. Armazene o texto em uma variável\n",
    "Aqui está a variável."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "Text = \"I love NLP and I will learn NLP in 2 month\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Etapa 3-2. Importar NLTK para POS\n",
    "Aqui está o código."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "[('I', 'PRP'),\n ('love', 'VBP'),\n ('NLP', 'NNP'),\n ('I', 'PRP'),\n ('learn', 'VBP'),\n ('NLP', 'RB'),\n ('2', 'CD'),\n ('month', 'NN')]"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Tokenize o texto\n",
    "tokens = sent_tokenize(Text)\n",
    "\n",
    "# Generate tagging for all the tokens using loop\n",
    "for i in tokens:\n",
    "    words = nltk.word_tokenize(i)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    # POS-tagger.\n",
    "    tags = nltk.pos_tag(words)\n",
    "tags"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A seguir estão os formulários curtos e as explicações da marcação de POV. A palavra *love* é VBP, que significa verbo, cantar. presente, tomada não 3D.\n",
    "* Conjunção coordenadora CC\n",
    "* CD dígitos cardinais\n",
    "* DT determinante\n",
    "* EX existencial lá (por exemplo, existência de algo)\n",
    "* FW palavra estrangeira\n",
    "* IN preposição/conjunção subordinativa\n",
    "* Adjetivo JJ, por exemplo: *big*\n",
    "* Adjetivo JJR, comparativo, por exemplo: *bigger*\n",
    "* Adjetivo JJS, superlativo, por exemplo: *bigest*\n",
    "* Marcador de lista LS 1)\n",
    "* MD modal could, *will*\n",
    "* NN substantivo, singular *desk*\n",
    "* NNS substantivo plural *desks*\n",
    "* NNP substantivo próprio, singular *Harrison*\n",
    "* NNPS nome próprio, plural *Americans*\n",
    "* PDT predetermina *all the kids*\n",
    "* POS pais da terminação possessiva\n",
    "* PRP pronome pessoal *I*, *he*, *she*\n",
    "* PRP$ pronome possessivo *my*, *his*, *her*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* advérbio RB, *silenty*\n",
    "* advérbio RBR, comparativo *better*\n",
    "* RBS advérbio, superlative *best*\n",
    "* Partícula RP *give up*\n",
    "* TO to go *to* the store\n",
    "* UH interjeição\n",
    "* VB verb, base form *take*\n",
    "* VBD verb, past tense *took*\n",
    "* VBG verb, gerund/present particíple *taking*\n",
    "* VBN verb, past particípio *taken*\n",
    "* VBP verbo, cantar, presente.não 3D. *take*\n",
    "* Verbo VBZ, 3ª pessoa *takes*\n",
    "* WDT wh-determinador *which*\n",
    "* WP wh-pronome *who, what*\n",
    "* WP$ possessivo wh-pronome *whose*\n",
    "* WRB wh-advérbio *where, when*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Receita 4-4. Extraindo Entidades do Texto\n",
    "Esta receita discute como identificar e extrair entidades do texto, o que é chamado de reconhecimento de entidade nomeada. Várias bibliotecas executam essa tarefa, como NLTK chunker, Stanford NER, spaCy, OpenNLP e NeuroNER. E há muitas APIs, como Watson NLU, AlchemyAPI, NERD, Google Cloud Natural Language API e muito mais.\n",
    "### Problema\n",
    "Você deseja identificar e extrair entidades do texto.\n",
    "### Solução\n",
    "A maneira mais simples de fazer isso é usando o ne_chunk do NLTK ou spaCy.\n",
    "### Como funciona\n",
    "Siga as etapas nesta seção para executar o NER.\n",
    "#### Etapa 4-1. Ler/criar os dados de texto\n",
    "Este é o texto."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "sent = \"John is studying at Stanford University in California\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Usando NLTK"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "Tree('S', [Tree('PERSON', [('John', 'NNP')]), ('is', 'VBZ'), ('studying', 'VBG'), ('at', 'IN'), Tree('ORGANIZATION', [('Stanford', 'NNP'), ('University', 'NNP')]), ('in', 'IN'), Tree('GPE', [('California', 'NNP')])])",
      "image/svg+xml": "<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight:normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,520.0,168.0\" width=\"520px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">S</text></svg><svg width=\"12.3077%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">PERSON</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">John</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"6.15385%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"7.69231%\" x=\"12.3077%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">is</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VBZ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"16.1538%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"15.3846%\" x=\"20%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">studying</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VBG</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"27.6923%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"6.15385%\" x=\"35.3846%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">at</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"38.4615%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"33.8462%\" x=\"41.5385%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">ORGANIZATION</text></svg><svg width=\"45.4545%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Stanford</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"22.7273%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"54.5455%\" x=\"45.4545%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">University</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"72.7273%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"58.4615%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"6.15385%\" x=\"75.3846%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">in</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"78.4615%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"18.4615%\" x=\"81.5385%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">GPE</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">California</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"90.7692%\" y1=\"1.2em\" y2=\"3em\" /></svg>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import libraries\n",
    "import nltk\n",
    "from nltk import ne_chunk\n",
    "from nltk import word_tokenize\n",
    "#NER\n",
    "ne_chunk(nltk.pos_tag(word_tokenize(sent)), binary=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Usando spaCy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple 0 5 ORG\n",
      "10000 42 47 MONEY\n",
      "New york 51 59 GPE\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(r'D:\\WILLIAN\\Anaconda3\\envs\\NLP_Recipes\\Lib\\site-packages\\en_core_web_sm\\en_core_web_sm-3.4.1')\n",
    "\n",
    "# Read/create a sentence\n",
    "doc = nlp(u'Apple is ready to launch new phone worth $10000 in New york time square')\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "De acordo com a saída, a Apple é uma organização, 10.000 é dinheiro e Nova York é um lugar. Os resultados são precisos e podem ser usados em qualquer aplicação de NLP.\n",
    "\n",
    "## Receita 4-5. Extraindo Tópicos do Texto\n",
    "Esta receita discute como identificar tópicos do documento. Por exemplo, existe uma biblioteca online com vários departamentos com base no tipo/gênero do livro. Você analisa palavras-chave/tópicos exclusivos para decidir a qual departamento este livro provavelmente pertence e o coloca de acordo. Nesses tipos de situações, a modelagem de tópicos seria útil. É chamado de marcação e agrupamento de documentos.\n",
    "### Problema\n",
    "Você deseja extrair ou identificar tópicos de um documento.\n",
    "### Solução\n",
    "A maneira mais simples é usar a biblioteca gensim.\n",
    "### Como funciona\n",
    "Siga as etapas nesta seção para identificar tópicos em documentos usando gensim.\n",
    "#### Etapa 5-1. Crie os dados de texto\n",
    "Aqui está o texto."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "['I am learning NLP, it is very interesting and exciting. it includes machine learning and deep learning',\n 'My father is a data scientist and he is nlp expert',\n 'My sister has good exposure into android development']"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1 = \"I am learning NLP, it is very interesting and exciting. it includes machine learning and deep learning\"\n",
    "doc2 = \"My father is a data scientist and he is nlp expert\"\n",
    "doc3 = \"My sister has good exposure into android development\"\n",
    "doc_complete = [doc1, doc2, doc3]\n",
    "doc_complete"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Etapa 5-2. Limpe e pré-processe os dados"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "[['learning',\n  'nlp',\n  'interesting',\n  'exciting',\n  'includes',\n  'machine',\n  'learning',\n  'deep',\n  'learning'],\n ['father', 'data', 'scientist', 'nlp', 'expert'],\n ['sister', 'good', 'exposure', 'android', 'development']]"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# Processando o texto\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = \"\".join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "doc_clean = [clean(doc).split() for doc in doc_complete]\n",
    "doc_clean"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Etapa 5-3. Prepare a matriz de termos do documento\n",
    "A seguir está o código."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 3), (5, 1), (6, 1)],\n [(6, 1), (7, 1), (8, 1), (9, 1), (10, 1)],\n [(11, 1), (12, 1), (13, 1), (14, 1), (15, 1)]]"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# Criação do dicionário de termos do nosso corpus, onde cada termo único recebe um índice.\n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "# Converter uma lista de documentos (corpus) em Document-Term Matrix usando o dicionário preparado acima.\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "doc_term_matrix"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Etapa 5-4. Criar o modelo LDA\n",
    "A parte final cria o modelo LDA."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.129*\"sister\" + 0.129*\"good\" + 0.129*\"exposure\" + 0.129*\"development\" + 0.129*\"android\" + 0.032*\"father\" + 0.032*\"scientist\" + 0.032*\"data\" + 0.032*\"expert\" + 0.032*\"nlp\"'), (1, '0.173*\"learning\" + 0.121*\"nlp\" + 0.069*\"deep\" + 0.069*\"interesting\" + 0.069*\"machine\" + 0.069*\"includes\" + 0.069*\"exciting\" + 0.069*\"scientist\" + 0.069*\"data\" + 0.069*\"father\"'), (2, '0.063*\"father\" + 0.063*\"data\" + 0.063*\"scientist\" + 0.063*\"expert\" + 0.063*\"nlp\" + 0.062*\"includes\" + 0.062*\"exciting\" + 0.062*\"interesting\" + 0.062*\"machine\" + 0.062*\"deep\"')]\n"
     ]
    }
   ],
   "source": [
    "# Criando o objeto para o modelo LDA usando a biblioteca gensim\n",
    "lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Rodando e treinando LDA no documento da matriz por três topicos.\n",
    "ldamodel = lda(doc_term_matrix, num_topics=3, id2word=dictionary, passes=50)\n",
    "# Resultados\n",
    "print(ldamodel.print_topics())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Todos os pesos associados aos tópicos da frase parecem quase semelhantes. Você pode fazer isso em documentos grandes para extrair tópicos importantes. A ideia de implementar isso em dados de amostra é torná-lo familiarizado com eles, e você pode usar o mesmo snippet de código para realizar grandes dados para obter resultados e insights significativos.\n",
    "\n",
    "## Receita 4-6. Classificando o Texto\n",
    "A classificação de texto classifica automaticamente documentos de texto com base em categorias pré-treinadas. Tem as seguintes aplicações.\n",
    "* Análise de sentimentos\n",
    "* Classificação de documentos\n",
    "* Classificação de spam/ham mail\n",
    "* Classificação de reclamações\n",
    "* Classificação de produto\n",
    "* Detecção de notícias falsas\n",
    "### Problema\n",
    "Classificação de spam/ham usando aprendizado de máquina.\n",
    "### Solução\n",
    "O Gmail tem uma pasta chamada Spam. Ele classifica seus e-mails em spam e ham para que você não precise ler e-mails desnecessários.\n",
    "### Como funciona\n",
    "Siga o método passo a passo para construir o classificador."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Etapa 6-1. Colete e entenda os dados"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['v1', 'v2', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], dtype='object')"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Lendo os dados\n",
    "Email_Data = pd.read_csv('spam.csv', encoding='latin1')\n",
    "\n",
    "# Entendendo os dados\n",
    "Email_Data.columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "  Target                                              Email\n0    ham  Go until jurong point, crazy.. Available only ...\n1    ham                      Ok lar... Joking wif u oni...\n2   spam  Free entry in 2 a wkly comp to win FA Cup fina...\n3    ham  U dun say so early hor... U c already then say...\n4    ham  Nah I don't think he goes to usf, he lives aro...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Target</th>\n      <th>Email</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ham</td>\n      <td>Go until jurong point, crazy.. Available only ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ham</td>\n      <td>Ok lar... Joking wif u oni...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>spam</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>U dun say so early hor... U c already then say...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>Nah I don't think he goes to usf, he lives aro...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Email_Data = Email_Data[['v1','v2']]\n",
    "Email_Data = Email_Data.rename(columns={'v1':'Target',\n",
    "                                        'v2':'Email'})\n",
    "Email_Data.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Etapa 6-2. Processamento de texto e engenharia de recursos\n",
    "A seguir está o código."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "  Target                                              Email\n0    ham  go jurong point, crazy.. avail bugi n great wo...\n1    ham                        ok lar... joke wif u oni...\n2   spam  free entri 2 wkli comp win fa cup final tkt 21...\n3    ham          u dun say earli hor... u c alreadi say...\n4    ham              nah think goe usf, live around though",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Target</th>\n      <th>Email</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ham</td>\n      <td>go jurong point, crazy.. avail bugi n great wo...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ham</td>\n      <td>ok lar... joke wif u oni...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>spam</td>\n      <td>free entri 2 wkli comp win fa cup final tkt 21...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>u dun say earli hor... u c alreadi say...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>nah think goe usf, live around though</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from textblob import TextBlob\n",
    "from nltk.stem import PorterStemmer\n",
    "from textblob import Word\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "import sklearn.feature_extraction.text as text\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "\n",
    "#etapas de pré-processamento como letras minúsculas, lematização e lematização\n",
    "Email_Data['Email'] = Email_Data['Email'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "stop = stopwords.words('english')\n",
    "Email_Data['Email'] = Email_Data['Email'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "st = PorterStemmer()\n",
    "Email_Data['Email'] = Email_Data['Email'].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n",
    "Email_Data['Email'] = Email_Data['Email'].apply(lambda x: \" \".join([Word\n",
    "(word).lemmatize() for word in x.split()]))\n",
    "Email_Data.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.52225884, 0.47751253, 0.31365376, ..., 0.35762099, 0.29247274,\n       0.3391114 ])"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting data into train and validation\n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(Email_Data['Email'], Email_Data['Target'])\n",
    "\n",
    "# TFIDF feature generation for a maximum of 5000 features\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(Email_Data['Email'])\n",
    "xtrain_tfidf = tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf = tfidf_vect.transform(valid_x)\n",
    "xtrain_tfidf.data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Etapa 6-3. Modelo de treinamento\n",
    "Esta é a função generalizada para treinar qualquer modelo."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9842067480258435\n"
     ]
    }
   ],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    return metrics.accuracy_score(predictions, valid_y)\n",
    "\n",
    "# Naive Bayes trainig\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(alpha=0.2), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print (\"Accuracy: \", accuracy)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9626704953338119\n"
     ]
    }
   ],
   "source": [
    "# Linear Classifier on Word Level TF IDF Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print (\"Accuracy: \", accuracy)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "O classificador Naive Bayes fornece melhores resultados do que o classificador linear. Você deve tentar alguns outros classificadores e depois escolher o melhor.\n",
    "\n",
    "## Receita 4-7. Realizando Análise de Sentimentos\n",
    "Esta receita discute o sentimento de uma determinada frase ou declaração. A análise de sentimentos é uma das técnicas amplamente utilizadas nas indústrias para entender os sentimentos dos clientes/usuários em relação aos produtos/serviços. A análise de sentimento fornece a pontuação de sentimento de uma frase/declaração tendendo para positivo ou negativo.\n",
    "### Problema\n",
    "Você quer fazer uma análise de sentimento.\n",
    "### Solução\n",
    "A maneira mais simples é usar TextBlob ou VADER.\n",
    "### Como funciona\n",
    "Siga as etapas nesta seção para fazer análise de sentimento usando TextBlob. Tem duas métricas.\n",
    "* A polaridade está no intervalo de [–1,1], onde 1 significa uma afirmação positiva e –1 significa uma afirmação negativa.\n",
    "* Subjetividade [0,1] é uma opinião e não informação factual.\n",
    "#### Etapa 7-1. Criar os dados de amostra\n",
    "Aqui estão os dados de amostra."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "review = \"I like this phone. screen quality and camera clarity is really good.\"\n",
    "review2 = \"This tv is not good. Bad quality, no clarity, worst experience\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Etapa 7-2. Limpe e pré-processe os dados\n",
    "\n",
    "Consulte o Capítulo 2, Receita 2-10, para esta etapa."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Etapa 7-3. Obtenha as pontuações de sentimento\n",
    "Use um TextBlob pré-treinado para obter as pontuações de sentimento."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "Sentiment(polarity=0.7, subjectivity=0.6000000000000001)"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "#TextBlob has a pre trained sentiment prediction model\n",
    "blob = TextBlob(review)\n",
    "blob.sentiment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Parece uma review positivo. (0.7)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "Sentiment(polarity=-0.6833333333333332, subjectivity=0.7555555555555555)"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob = TextBlob(review2)\n",
    "blob.sentiment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Claramente é um review negativo. -0.68."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> Observe que um caso de uso em tempo real na análise de sentimento com uma implementação de ponta a ponta é abordado na receita 5-2 no próximo capítulo.\n",
    "\n",
    "## Receita 4-8. Texto desambiguado\n",
    "A ambigüidade surge devido aos diferentes significados das palavras em um contexto diferente. Por exemplo,"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "Text1 = 'I went to the bank to deposit my money'\n",
    "Text2 = 'The river bank was full of dead fish'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nos textos, a palavra banco tem significados diferentes de acordo com o contexto da frase.\n",
    "### Problema\n",
    "Você quer entender o sentido da palavra sem ambiguidade.\n",
    "### Solução\n",
    "O algoritmo Lesk é um dos melhores algoritmos para desambiguação de sentido de palavra. Vejamos como resolvê-lo usando os pacotes pywsd e nltk.\n",
    "### Como funciona\n",
    "A seguir estão as etapas para alcançar os resultados.\n",
    "#### Etapa 8-1. Importar bibliotecas\n",
    "Primeiro, importe as bibliotecas."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warming up PyWSD (takes ~10 secs)... took 1.9308173656463623 secs.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import PorterStemmer\n",
    "from itertools import chain\n",
    "from pywsd.lesk import simple_lesk"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Etapa 8-2. Desambiguar o sentido da palavra\n",
    "Aqui está o código."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context-1: I went to the bank to deposit my money\n",
      "Sense: Synset('depository_financial_institution.n.01')\n",
      "Definition :  a financial institution that accepts deposits and channels the money into lending activities\n",
      "Context-2: The river bank was full of dead fishes\n",
      "Sense: Synset('bank.n.01')\n",
      "Definition :  sloping land (especially the slope beside a body of water)\n"
     ]
    }
   ],
   "source": [
    "# Sentences\n",
    "bank_sents = ['I went to the bank to deposit my money', 'The river bank was full of dead fishes']\n",
    "\n",
    "# calling the lesk function and printing results for both the sentences\n",
    "print (\"Context-1:\", bank_sents[0])\n",
    "answer = simple_lesk(bank_sents[0],'bank')\n",
    "print (\"Sense:\", answer)\n",
    "print (\"Definition : \", answer.definition())\n",
    "print (\"Context-2:\", bank_sents[1])\n",
    "answer = simple_lesk(bank_sents[1],'bank','n')\n",
    "print (\"Sense:\", answer)\n",
    "print (\"Definition : \", answer.definition())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Observe que no contexto-1, “banco” é uma instituição financeira, mas no contexto-2, “banco” é um terreno inclinado.\n",
    "## Receita 4-9. Convertendo fala em texto\n",
    "A conversão de fala em texto é uma técnica de PNL muito útil.\n",
    "### Problema\n",
    "Você deseja converter fala em texto.\n",
    "### Solução\n",
    "A maneira mais simples é usar o reconhecimento de fala e o PyAudio.\n",
    "### Como funciona\n",
    "Siga as etapas nesta seção para implementar a conversão de fala em texto.\n",
    "#### Etapa 9-1. Defina o problema do negócio\n",
    "A interação com as máquinas está tendendo para a voz, que é a forma usual de comunicação humana. Exemplos populares são Apple Siri, Amazon Alexa e Google Home.\n",
    "\n",
    "#### Etapa 9-2. Instale e importe as bibliotecas necessárias\n",
    "Aqui estão as bibliotecas."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "import speech_recognition as sr"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Etapa 9-3. Execute o código\n",
    "Agora, depois de executar o trecho de código a seguir, tudo o que você disser no microfone (usando a função *recognize_google*) será convertido em texto."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please say something\n",
      "Time over, thanks\n",
      "I think you said: my name is William\n"
     ]
    }
   ],
   "source": [
    "r=sr.Recognizer()\n",
    "with sr.Microphone() as source:\n",
    "    print(\"Please say something\")\n",
    "    audio = r.listen(source)\n",
    "    print(\"Time over, thanks\")\n",
    "try:\n",
    "    print(\"I think you said: \"+r.recognize_google(audio))\n",
    "except:\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Este código funciona com o idioma inglês padrão. Se você fala outro idioma, por exemplo, Português, o texto é interpretado em inglês, conforme a seguir."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please say something\n",
      "Time over, thanks\n",
      "I think you said: bowling league\n"
     ]
    }
   ],
   "source": [
    "#code snippet\n",
    "r=sr.Recognizer()\n",
    "with sr.Microphone() as source:\n",
    "    print(\"Please say something\")\n",
    "    audio = r.listen(source)\n",
    "    print(\"Time over, thanks\")\n",
    "try:\n",
    "    print(\"I think you said: \"+r.recognize_google(audio));\n",
    "except:\n",
    "    pass;"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Se você deseja que o texto apareça no idioma falado, execute o seguinte trecho de código. Uma pequena alteração foi feita em recognize_google –language ('pt-BR', que significa Português brasileiro)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please say something\n",
      "Time over, thanks\n",
      "I think you said: agora funciona em português\n"
     ]
    }
   ],
   "source": [
    "#code snippet\n",
    "r=sr.Recognizer()\n",
    "with sr.Microphone() as source:\n",
    "    print(\"Please say something\")\n",
    "    audio = r.listen(source)\n",
    "    print(\"Time over, thanks\")\n",
    "\n",
    "try:\n",
    "    print(\"I think you said: \"+r.recognize_google(audio, language ='pt-BR'));\n",
    "except sr.UnknownValueError:\n",
    "    print(\"Google Speech Recognition could not understand audio\")\n",
    "except sr.RequestError as e:\n",
    "    print(\"Could not request results from Google Speech Recognition service; {0}\".format(e))\n",
    "except:\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Receita 4-10. Convertendo Texto em Fala\n",
    "A conversão de texto em fala é outra técnica útil de PNL.\n",
    "### Problema\n",
    "Você deseja converter texto em fala.\n",
    "### Solução\n",
    "A maneira mais simples é usar a biblioteca gTTs.\n",
    "### Como funciona\n",
    "Siga as etapas nesta seção para implementar a conversão de texto em fala.\n",
    "#### Etapa 10-1. Instale e importe as bibliotecas necessárias\n",
    "Aqui estão as bibliotecas."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "from gtts import gTTS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Etapa 10-2. Execute o código com a função gTTs\n",
    "Agora, depois de executar o trecho de código a seguir, tudo o que você inserir no parâmetro de texto será convertido em áudio."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "#escolhe o idioma, Portugês\n",
    "convert = gTTS(text='Eu gosto de estudar!', lang=\"pt-BR\", slow=False)\n",
    "convert.save('AUDIO.MP3')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
